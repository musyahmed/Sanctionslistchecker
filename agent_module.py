#!/usr/bin/env python3
"""
Enhanced Interactive Voice-&-File-Enabled Personal Banking Agent for Morgan J. Reynolds:
 - Personal details loaded from a Markdown file
 - OpenAI Whisper-1 ASR (audio→text via API)
 - GPT-4 powered conversational intelligence
 - Google Cloud TTS for all responses
 - Fuzzy matching and clarification for ambiguous queries
 - Conversation logging to session_log.json
 - Always addresses user as Morgan J. Reynolds
"""

import os
import json
import tempfile
import subprocess
import re
from datetime import datetime

import sounddevice as sd
import soundfile as sf
import numpy as np
from openai import OpenAI
from google.cloud import texttospeech
from cryptography.fernet import Fernet, InvalidToken
from real_vector_db_client import VectorDBClient # Added import

# ── CONFIG ────────────────────────────────────────────────────────────────────
OPENAI_KEY    = os.getenv("OPENAI_API_KEY")
if not OPENAI_KEY:
    raise RuntimeError("Please set OPENAI_API_KEY")
# Ensure GOOGLE_APPLICATION_CREDENTIALS is set to service account JSON

CHAT_MODEL     = "gpt-4" # CHAT_MODEL could be set via an environment variable for more flexibility.
RECORD_SEC     = 5         # seconds to record
SAMPLE_RATE    = 16000     # mic sample rate
LOG_PATH       = "session_log.json"
VECTOR_DB_URI  = os.getenv("VECTOR_DB_URI", "mock_db")
# ── END CONFIG ─────────────────────────────────────────────────────────────────

# Initialize clients
openai_client = OpenAI(api_key=OPENAI_KEY)
tts_client    = texttospeech.TextToSpeechClient()
# Initialize vector_db_client based on VECTOR_DB_URI
vector_db_client = VectorDBClient(VECTOR_DB_URI) if VECTOR_DB_URI != "mock_db" else None

# --- Conceptual Field-Level Encryption Setup ---
def get_secure_encryption_key() -> bytes:
    # In production, this function would securely fetch the key from a
    # Key Management Service (KMS) or other secure secret store.
    key_from_env = os.getenv("ENCRYPTION_KEY")
    if key_from_env:
        print("INFO: Loaded encryption key from ENCRYPTION_KEY environment variable.")
        # Ensure the key from env is bytes. If it's base64 encoded string, decode it.
        # For Fernet, keys must be 32 url-safe base64-encoded bytes.
        # Assuming the env var stores the key in the format Fernet.generate_key() produces (already url-safe base64 encoded bytes, then utf-8 decoded for env var)
        return key_from_env.encode()
    else:
        print("WARNING: ENCRYPTION_KEY environment variable not set.")
        print("WARNING: Generating a new ephemeral encryption key for this session.")
        print("WARNING: Any data encrypted in previous sessions with a different key WILL BE UNRECOVERABLE.")
        print("WARNING: For persistent encrypted data, ENCRYPTION_KEY MUST be set consistently.")
        new_key = Fernet.generate_key()
        print(f"INFO: Generated new ephemeral key: {new_key.decode()} (This is for info only, normally key is not printed)")
        return new_key

_ENCRYPTION_KEY = get_secure_encryption_key()
fernet = Fernet(_ENCRYPTION_KEY)

def encrypt_data(data: str) -> str:
    """Encrypts string data."""
    return fernet.encrypt(data.encode()).decode()

def decrypt_data(encrypted_data: str) -> str | None:
    """Decrypts string data. Returns None on error."""
    try:
        # Actual decryption logic for real encrypted data
        return fernet.decrypt(encrypted_data.encode()).decode()
    except InvalidToken:
        print(f"Decryption failed for token: '{encrypted_data[:20]}...' due to InvalidToken (likely wrong key or corrupted data).")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during decryption for token '{encrypted_data[:20]}...': {e}")
        return None

# ── Mock Vector DB Data and Accessor ───────────────────────────────────────────
# Conceptual: Sensitive fields are stored "encrypted".
# For this demo, we use placeholder strings that decrypt_data will recognize.
# In a real scenario, these would be actual ciphertexts generated by encrypt_data.
# mock_vector_db_data has been removed. Data now comes from VectorDBClient.

def get_user_data_from_vector_db(user_id: str, db_client_instance: any) -> dict: # Renamed db_client to db_client_instance for clarity
    """
    Retrieves user data. If db_client_instance is provided (real DB mode), uses it.
    If db_client_instance is None (mock_db mode, as VECTOR_DB_URI="mock_db"), returns empty data with a warning.
    Decrypts specified fields if they exist in the retrieved data.
    """
    retrieved_data = {}
    if db_client_instance: # Real DB client provided
        print(f"Using real VectorDBClient. Fetching data for user_id '{user_id}'...")
        # Make a copy to modify for decryption, if get_user_data returns a mutable dict.
        # If it returns an immutable dict or a copy already, .copy() might not be needed.
        retrieved_data = db_client_instance.get_user_data(user_id)
        if not isinstance(retrieved_data, dict): # Ensure it's a dict for .copy()
            print(f"Warning: VectorDBClient.get_user_data did not return a dict for user {user_id}. Received: {type(retrieved_data)}")
            return {} # Return empty if not a dict
        retrieved_data = retrieved_data.copy()

        if retrieved_data:
            print(f"Data found for user_id '{user_id}' via VectorDBClient. Proceeding with decryption...")
        else:
            print(f"No data found for user_id '{user_id}' via VectorDBClient.")
            return {} # Return empty if no data found

    elif VECTOR_DB_URI == "mock_db": # db_client_instance is None because we are in mock_db mode
        print("WARNING: Operating in mock DB mode for get_user_data_from_vector_db, but mock_vector_db_data is removed. Returning empty data.")
        return {} # No mock_vector_db_data to fall back to

    else: # Should not happen if vector_db_client is initialized correctly based on VECTOR_DB_URI
        print(f"Error: db_client_instance is None, but VECTOR_DB_URI ('{VECTOR_DB_URI}') is not 'mock_db'.")
        return {}

    # Common decryption logic for data from any source (real DB or future mocks)
    if retrieved_data: # Ensure data was actually retrieved before trying to decrypt
        # Decrypt sensitive fields
        fields_to_decrypt = ["account_number_encrypted", "tax_id_encrypted"] # Add other encrypted fields here
        for field_name in fields_to_decrypt:
            if field_name in retrieved_data:
                encrypted_value = retrieved_data[field_name]
                decrypted_value = decrypt_data(encrypted_value)
                if decrypted_value:
                    retrieved_data[field_name.replace("_encrypted", "")] = decrypted_value
                else:
                    print(f"Warning: Could not decrypt {field_name} for user {user_id}")
                    retrieved_data[field_name.replace("_encrypted", "")] = None # Or handle error as appropriate
                del retrieved_data[field_name] # Remove encrypted version from result

    return retrieved_data

# ── TTS Playback ────────────────────────────────────────────────────────────────
def speak(text: str):
    try:
        synthesis_input = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(
            language_code="en-US",
            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE
        )
        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)
        response = tts_client.synthesize_speech(
            input=synthesis_input, voice=voice, audio_config=audio_config
        )
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as out:
            out.write(response.audio_content)
            tmp_path = out.name
        subprocess.run(["afplay", tmp_path], check=False)
    except Exception as e:
        print(f"TTS Error: {e}")
        try:
            subprocess.run(["say", text], check=False)
        except:
            print("System TTS fallback failed.")

# ── ASR Helpers ────────────────────────────────────────────────────────────────
def transcribe_file(path: str) -> str:
    resp = openai_client.audio.transcriptions.create(
        model="whisper-1", file=open(path, "rb"), language="en", temperature=0.0
    )
    return resp.text


def record_audio(seconds=RECORD_SEC, sr=SAMPLE_RATE) -> str:
    print(f"Recording {seconds}s… please speak now.")
    audio = sd.rec(int(seconds*sr), samplerate=sr, channels=1, dtype="int16")
    sd.wait()
    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    sf.write(tmp.name, audio, sr)
    return tmp.name

# ── Conversation & Q&A ─────────────────────────────────────────────────────────
def ask_llm(history, query, user_name: str, user_details: dict):
    context_from_db = "Relevant account information: "
    initial_context_len = len(context_from_db) # Length of the initial part

    if user_details.get('account_type'): # Existing context
        context_from_db += f"Your account type is {user_details['account_type']}. "

    # New richer context from VectorDB (as per real_vector_db_client.py)
    if user_details.get("recent_transaction_summaries_from_db"):
        context_from_db += f"Recent transaction insights: {user_details['recent_transaction_summaries_from_db']}. "
    if user_details.get("relevant_faq_snippets_from_db"):
        context_from_db += f"Related FAQ information: {user_details['relevant_faq_snippets_from_db']}. "

    # Add other new contextual fields as needed from user_details if they exist
    # Example:
    # if user_details.get("preferred_communication_channel"):
    #     context_from_db += f"Preferred contact method: {user_details['preferred_communication_channel']}. "

    # Add detailed transaction history if available
    if user_details.get("detailed_transactions"):
        transactions_str = " Detailed transaction history (most recent first, up to 10): "
        # Sort transactions by date, most recent first
        try:
            sorted_transactions = sorted(
                user_details["detailed_transactions"],
                key=lambda x: x.get("date", "0000-00-00"), # Use a default sort key for missing dates
                reverse=True
            )[:10] # Limit to top 10

            for tx in sorted_transactions:
                amount = tx.get('amount', 0.0)
                # Ensure amount is float for formatting, handle potential string values if any
                try:
                    amount_val = float(amount)
                except (ValueError, TypeError):
                    amount_val = 0.0

                amount_str = f"${abs(amount_val):.2f}"
                if amount_val < 0:
                    amount_str = f"(-{amount_str})" # Indicate outgoing
                else:
                    amount_str = f"(+{amount_str})" # Indicate incoming

                transactions_str += f"{tx.get('date', 'N/A')} - {tx.get('description', 'N/A')} {amount_str}; "

            if transactions_str.endswith("; "): # Remove trailing semicolon and space
                transactions_str = transactions_str[:-2] + "."
            context_from_db += transactions_str
        except Exception as e: # Catch any error during transaction processing
            print(f"Error processing detailed_transactions: {e}")
            context_from_db += " Error processing transaction details. "


    # Fallback if no specific context is found other than the initial string
    if len(context_from_db) == initial_context_len: # Check if anything was added other than the initial part
         context_from_db = "Standard account context applies." # Overwrite if nothing specific found

    system_prompt = (
        f"You are the personal banking assistant for {user_name}. "
        f"Answer questions about balance, spending, account details, general queries, and provide details about recent transactions from the provided transaction history. " # Updated prompt
        f"Always address the user as {user_name}, be helpful, concise, and professional. "
        f"Use the following information to answer the user's questions: {context_from_db}"
    )
    messages = [{"role": "system", "content": system_prompt}]
    for ui, ai in history[-5:]:
        messages.append({"role": "user", "content": ui})
        messages.append({"role": "assistant", "content": ai})
    messages.append({"role": "user", "content": query})
    resp = openai_client.chat.completions.create(model=CHAT_MODEL, messages=messages)
    return resp.choices[0].message.content.strip()

# ── Main Loop ──────────────────────────────────────────────────────────────────
def main():
    # Prompt for User ID
    current_user_id = input("Please enter your User ID (e.g., 'user123', 'user456'): ").strip()

    # Load details using the provided User ID
    details = get_user_data_from_vector_db(current_user_id, vector_db_client)

    if not details:
        print("User ID not found. Exiting.")
        return

    # Seed conversation history
    history = []

    # Welcome
    user_name = details.get('name', 'Valued Customer') # Fallback name
    welcome = f"Hello {user_name}, how can I assist you with your banking today?"
    print(welcome)
    speak(welcome)
    history.append((welcome, ''))

    while True:
        print("\nOptions: 1) Audio File  2) Live Recording  3) Type  4) Quit")
        choice = input("Choice [1-4]: ").strip().lower()
        if choice in ('4','quit','exit'):
            break

        # Get query text
        if choice == '1':
            path = input("Audio file path: ").strip()
            if not os.path.isfile(path): continue
            text = transcribe_file(path)
        elif choice == '2':
            wav = record_audio()
            text = transcribe_file(wav)
        else:
            text = input("Your question: ").strip()
        if not text:
            continue

        # Answer (always use ask_llm now)
        ans = ask_llm(history, text, user_name, details) # 'details' should be the user_details from get_user_data_from_vector_db

        print(ans)
        speak(ans)
        history.append((text, ans))

    goodbye = f"Goodbye {user_name}. Have a great day!"
    print(goodbye)
    speak(goodbye)
    # Log session
    with open(LOG_PATH, 'w', encoding='utf-8') as f:
        json.dump([{'user':u,'assistant':a,'timestamp':datetime.now().isoformat()} for u,a in history], f, indent=2)

if __name__ == '__main__':
    main()
